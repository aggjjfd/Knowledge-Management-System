> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [www.zhihu.com](https://www.zhihu.com/question/263672028/answer/2250289371) ![](https://pic1.zhimg.com/v2-bbb73a3bd4f47d92f4e70086f09bb930_xs.jpg?source=1940ef5c)Moenova

这是一个很简单的[信息论](https://www.zhihu.com/search?q=%E4%BF%A1%E6%81%AF%E8%AE%BA&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2250289371%7D)知识，信息不可能无限压缩。

我们为什么会觉得所有的真理都可以被压缩，或者说都可以建模成人类能理解的样子呢？

人类理想中的世界是真理都可以被压缩成几句简单的语言，然后凭借几句 “真言” 就可以了解整个世界。

但这种幻想是不切实际的，世界本来就是一个黑箱，只不过人类找出了其中的某些规律，但除此之外还有很多是人类所不能理解的。

用黑箱，或者函数来比喻神经网络其实并没有触及到神经网络的本质。

神经网络的本质是纯粹的信息流和纯粹的逻辑，只是用函数的方式实现了出来（如果不用函数，那么就很难求导，找寻极值点会变得很困难）

你可以这么理解神经网络，神经网络是一个只会用简单运算符号的程序员，每当程序出错，神经网络就在程序里加上一个 if 条件，在先开始的时候大家都能看懂神经网络在写什么。

但是随着需求越来越多，神经网络加了成千上万行的 if 语句，然后整个代码就变成了难以理解的屎山，但是却能很好的的解决所有见过的问题以及相似的问题。

神经网络中有一类特殊的函数激活函数，很多初学者难以理解激活到底意味着什么，好好的函数为啥要激活？还这么简单？为什么不是 sin，cos，log，等等稍微复杂点的函数呢？

因为激活函数只为完成一个简单的需求，用四则运算，模拟出 if 条件语句的效果。

很少有大学教授会把这个原理讲述出来，大部分学校老师只会念 PPT，他们只知道这个东西是什么，而不会从原理出发，理解当初第一个使用激活函数的人到底是出于什么目的使用激活函数。

简单的神经网络可以理解成线性运算，if 条件，然后如此循环。而激活函数也是 if 条件中最简单的一种就是，把负数归零。

你会问为什么要负数归零？而不是正数归零？因为效果其实是一样的，把任何区间的数归零，都是差不多的，在上一层线性符号反一下，就行了，结果差不多的。

重要的是归零这个操作而不是什么情况下归零，这个操作相当于 if 某某条件，直接返回 None。而不进行下一步的运算。

更进一步可以理解为，遇到这种情况直接忽略，那么没有被忽略的数值就是被抽取出来的信息。也就等于说神经网络用归零这个简单的操作来判断哪些信息重要，哪些不重要。

这也是反直觉的学习理论，学习并不是记住所有内容，而是去辨别信息的价值，忽略掉不重要的信息，以此抽取出重要的信息才是智慧的体现。

这就是很多记忆力很好的人，很难悟到事物本质的原因，因为他们只是在复制信息，并没有对信息做进一步的加工处理。

也就是说激活函数使得神经网络具备了信息遗忘的功能，而且它会根据线性运算来决定要不要忘掉这个信息。

没错神经网络对于很多人来说就是黑箱，但这种黑箱其实是有内在规律可循的。

比如你可以问问自己一种任务是否可以通过频繁的 test 和需求变更，让程序员去书写大量的 if 条件来解决。如果可以，那么你就可以使用神网络，在这种情况下，神经网络和你让一群程序员写 if 条件是没有本质区别的。反正屎山和黑箱都是一种东西，最后都没人能看懂。

但不是所有东西都可以用大量的 test 来解决，有很多时候你知道你要解决什么问题，这个问题对输入输出都非常明确，但没有很多的 test case。这种时候你就不能用神经网络，你需要使用专家系统，必要的时候用 formal method 去证明你的程序绝逼是没有 bug 的。比如航空航天，核弹控制器，飞机控制系统，[心脏起搏器](https://www.zhihu.com/search?q=%E5%BF%83%E8%84%8F%E8%B5%B7%E6%90%8F%E5%99%A8&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A2250289371%7D)等等。

[神经网络的本质 1——无限拟合函数本文将用一种直观的方式去理解神经网络 为了可视化，我们把整个网络简化到最简单的形式，也就是从一次函数 线性变换线性和非线性说起来有点抽象，但什么是线性变换呢？ 这个变换只能： 在竖直方向上拉升，反向，收缩 在竖直方向上上下移动。 注意：这个连横…![](https://pic1.zhimg.com/v2-6394d1a52bb67cdeeca3209ecad986c4_540x300.jpg?source=172ae18b)266 赞同 · 27 评论查看完整文章](https://zhuanlan.zhihu.com/p/432775491)[神经网络的本质 2 —— 更加丰富的非线性结构如果按照一般的结构，（线性层 -> 激活 -> 线性层 -> 激活），可能无法做到一些特别的函数，比如取绝对值，if else 等等。当然可以用循环神经网络，递归神经网络，卷积，池化，等等方法，不过这里讨论的是更加本质的内容。也就是如何去设计神经网络的结构，来…![](https://pic1.zhimg.com/v2-ebf75f82698e717a96d787ab7bf6804c_540x300.jpg?source=172ae18b)43 赞同 · 2 评论查看完整文章](https://zhuanlan.zhihu.com/p/434976034)

果然很多人文章都没看完就开始 bb 了。

很多人还在争论激活语句是不是 if 条件。

relu 的 y=max（0，x）

和 y= x if x＞0 else 0

这不就是一样的么？这也能杠？其他的激活函数无非是软化版的或者平移，拉伸过的，本质其实是都是一样的。

不知道这些人是怎么从事计算机行业的，没学过编程么？

![](https://pic1.zhimg.com/v2-b7d7b5617577d17dbdbaa796186145ca_xs.jpg?source=1940ef5c)Han Yang

没做过实际的数据挖掘项目，不过我觉得，可解释性应该指的是对于数据的可解释性。

比如你老板让你做一个预测房价的问题。给了你历年的房屋售价，和该房子的面积、地段、类型、建造年限、装修情况等特征。然后你把这些特征丢进一个机器学习模型去训练。训练好之后，用模型预测一下明年的房价，汇报给老板。

然后问题就解决了？

没有解决，因为你的老板会问你，为什么？为什么房价会上升（下降）？

这时，

如果你用的是 logistic regression，你就可以自信满满地指着 p-value 最小的那些变量，跟老板说，“因为这些变量变大（变小）了，所以明年房价会上升（下降）”

如果你用的是 decision tree，你就可以自信满满地指着决策树最顶端的结点，跟老板说，“因为这些变量变大（变小）了，所以明年房价会上升（下降）”

如果你用的是 NN，你就可以（自信满满？）地跟老板说，……

 ![](https://pic3.zhimg.com/3cc4116c7fe1a63303e2b7ca968ff75a_xs.jpg?source=1940ef5c) 月光宝盒娱乐频道

说神经网络是个黑箱，大致有两层意思。一个是说，我们不能刻画网络具体是在做什么（比如在每一层提取什么样的特征）。另一个是说，我们不知道它为什么在做这些，为什么会有效。回答前一个问题就是研究神经网络的 “可解释性”。不过在俺看来后者更重要，更本质。

有些人似乎认为，这两个问题都不重要。一种思潮是说只要好用就行，管它为什么工作、怎么工作的。还有一种思潮是认为这两个问题都是根本回答不了的。俺以为，这两种观点都是短视的。

深度学习显然是做对了 something, 在某个角度触碰到了真理，但如果不打开黑箱，我们无法知道它到底做对了什么，真理是什么。在牛顿之前，大家都见到了苹果落地。但在当时人们的视角中他们一定认为，苹果落地不是很自然的吗，需要解释、需要知道为什么吗？当时的人们也会认为解释这种现象简直无从下手。跟今天的深度学习有点像吧？但是当牛顿告诉我们为什么苹果会落地之后，世界从此就不一样了。

今天的机器学习在某种意义上好像物理学。深度学习目前就好比实验物理，有很多实验观察，但没有理论能够解释。也跟物理学的发展一样，实验总是走在前面。没有 Michelson／Morley 的实验观察，也就没有爱因斯坦的狭义相对论。

如果感兴趣深度学习的科研，你可以选择成为下一个 Michelson／Morley， 也可以选择成为下一个爱因斯坦。当然，大多数人都是坐在树下吃那些天经地义要掉下来的苹果，呵呵。